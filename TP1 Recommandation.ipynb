{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Recommandation engine\n",
    "\n",
    "This notebook was realized with python 3.6, and requires the files *P_als.txt* and *Q_als.txt* to be in the same repertory.\n",
    "\n",
    "### First, a few imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#File import + extraction\n",
    "import requests, zipfile, io\n",
    "from movielensutils import*\n",
    "\n",
    "#math tools\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.optimize import check_grad\n",
    "import numpy as np\n",
    "\n",
    "#time\n",
    "from time import time\n",
    "from progress_bar import InitBar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1) Introducing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Retrieving the dataset\n",
    "zip_url = \"http://files.grouplens.org/datasets/movielens/ml-100k.zip\"\n",
    "zip_file = requests.get(zip_url)\n",
    "zip_file = zipfile.ZipFile(io.BytesIO(zip_file.content))\n",
    "zip_file.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users and 1682 movies in the database\n",
      "Total amount of rankings n = 100000 \n"
     ]
    }
   ],
   "source": [
    "#Loading the dataset\n",
    "R, mask = load_movielens(\"ml-100k/u.data\")\n",
    "print(\"There are %d users and %d movies in the database\"%(R.shape[0], R.shape[1]))\n",
    "print(\"Total amount of rankings n = %d \"%(np.count_nonzero(mask)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The objective function to be minimized is\n",
    "$$ g : P,Q \\mapsto \\frac{1}{2}\\left|\\left|\n",
    "\\mathbb{1}_K \\circ (R - QP) \\right|\\right|_F^2 + \\frac{\\rho}{2}\\left|\\left|\n",
    "Q\\right|\\right|_F^2 + + \\frac{\\rho}{2}\\|\n",
    "P \\|_F^2$$ \n",
    "The gradient is \n",
    "$$ \\bigtriangledown g : P,Q \\mapsto \\begin{pmatrix}\n",
    "                    - Q^T\\left[\\mathbb{1}_K\\circ (R - QP)\\right] + \\rho P \\\\ \n",
    "                    - \\left[\\mathbb{1}_K\\circ (R - QP)\\right] P^T + \\rho Q\n",
    "                    \\end{pmatrix}\n",
    "$$ \n",
    "\n",
    "The Hessian matrix is... painful to interpret. To answer the question: \"is it a convex function?\" so that we know whether a minimum is global or not, we will try another approach. As the product QP is a good sign that it is NOT convex, we will show that the Hessian related to the corresponding 1D problem is not defined positive.\n",
    "\n",
    "Let us set $\\hat{g}(p,q) = \\frac{1}{2}(r-pq)^2$ with $\\left(r,p,q\\right) \\in \\mathbb{R}^3$ (because $\\frac{\\rho}{2}\\left(q^2+p^2\\right)$ is convex anyway).\n",
    "Then we have $\\triangledown \\hat{g}(p,q) =\n",
    "\\begin{pmatrix}\n",
    "                    - q(r-pq) \\\\ \n",
    "                    - p(r-pq)\n",
    "\\end{pmatrix}$\n",
    "$\\triangledown^2 \\hat{g}(p,q) =\n",
    "\\begin{pmatrix}\n",
    "                    q^2 && 2qp - r \\\\ \n",
    "                    2qp-r &&p^2\n",
    "\\end{pmatrix}$\n",
    "\n",
    "Therefore $\\begin{pmatrix}p & q \\end{pmatrix}\\triangledown^2 \\hat{g}\\begin{pmatrix} p \\\\ q \\end{pmatrix} = (px_1+qx_2)^2+2x_1x_2(qp-r)$. This not always positive, even with $x \\neq 0$. For example, with the set of values $\\big(x_1, x_2, p, q, r\\big) = \\big(1,1,1,1,3\\big)$, it is equal to 0, and with $r = 4$ instead, it is negative.\n",
    "\n",
    "$g$ is not lipschitzian because the hessian matrix coefficients are not limited to a specific range, but a polynomial expression of P and Q with degre = 2... It is therefore not lipschitzian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2) Find $P$ when $Q_0$ is fixed\n",
    "\n",
    "The function $ g : P \\mapsto \\frac{1}{2}\\left|\\left|\n",
    "\\mathbb{1}_K \\circ (R - Q^0P) \\right|\\right|_F^2 + \\frac{\\rho}{2}\\left|\\left|\n",
    "Q^0\\right|\\right|_F^2 + + \\frac{\\rho}{2}\\|\n",
    "P \\|_F^2$ has for gradient $\\bigtriangledown g(P) = -{Q^0}^T\\mathbb{1}_K \\circ (R - Q^0P) + \\rho P$ and for hessian matrix $\\bigtriangledown^2 g(P) = {Q^0}^T \\mathbb{1}_K \\mathbb{1}_K^T {Q^0} + \\rho I$ which is defined positive when $\\rho$ is carefully chosen. Therefore g is convex.\n",
    "\n",
    "**BEWARE** : the computing time of the next cell may take up to 7 min (check grad takes a while). I prefered to use the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16737625347 0.00152389552688\n"
     ]
    }
   ],
   "source": [
    "def objective(P, Q0, R, mask, rho):\n",
    "    \"\"\"\n",
    "    g in the problem (objective function)\n",
    "    \n",
    "    Input:\n",
    "    P : Matrix input with shape C x I\n",
    "    Q0 : matrix with shape  U x C\n",
    "    R : matrix with shape U x I\n",
    "    mask : binary matrix with shape U x I\n",
    "    rho : strictly positive real\n",
    "\n",
    "    Output :\n",
    "    val : g(P)\n",
    "    grad_P : obviously...\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q0.dot(P)) * mask\n",
    "\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q0 ** 2) + np.sum(P ** 2))\n",
    "    grad_P = -np.dot(Q0.T, tmp) + rho*P\n",
    "    return (val, grad_P)\n",
    "\n",
    "\n",
    "#Initialization of the parameters (R and mask are already defined in the previous cell)\n",
    "C = 6\n",
    "U, I = R.shape\n",
    "Q0,_,P0 = svds(R, k=C)\n",
    "rho = 0.2\n",
    "\n",
    "#Functions required for use in the check_grad function\n",
    "def func_g(P):\n",
    "    \"\"\"\n",
    "    same as objective, but the purpose is to use a vector as an argument and return only val\n",
    "    \"\"\"\n",
    "    global Q0, R, mask, rho, C\n",
    "    PP = np.reshape(P, (C, I))\n",
    "    return objective(PP, Q0, R, mask, rho)[0]\n",
    "\n",
    "def func_g_grad(P):\n",
    "    \"\"\"\n",
    "    same as objective, but the purpose is to use a vector as an argument and return only grad_P\n",
    "    \"\"\"\n",
    "    global Q0, R, mask, rho, C\n",
    "    PP = np.reshape(P, (C, I))\n",
    "    return np.ravel(objective(PP, Q0, R, mask, rho)[1])\n",
    "\n",
    "#checking the gradiant precision\n",
    "error = check_grad(func_g, func_g_grad, np.ravel(P0))\n",
    "relative_error = error / np.linalg.norm(objective(P0, Q0, R, mask, rho)[1])\n",
    "print(error, relative_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The error could be lower, but it is still ok as the relative error is low.\n",
    "\n",
    "Now let us code a fixed step gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed step gradient descent : 43 steps, time elapsed 1.854 s\n"
     ]
    }
   ],
   "source": [
    "def gradient(f_and_grad, P0, gamma, epsilon):\n",
    "    \"\"\"\n",
    "    executes a gradient descent with fixed step\n",
    "    \n",
    "    Input:\n",
    "    f_and_grad is a tuple : returns both function and its gradient.\n",
    "    P0 = initialization point\n",
    "    epsilon : threshold for gradient variation\n",
    "    \"\"\"\n",
    "    P = P0.copy()\n",
    "    n = 0\n",
    "    grad = f_and_grad(P)[1]\n",
    "    while np.linalg.norm(grad) > epsilon and n < 300 :\n",
    "        grad = f_and_grad(P)[1]\n",
    "        P = P - gamma*grad\n",
    "        n+=1\n",
    "    return P,n    \n",
    "\n",
    "#Computing the constant from the lipschitzian function inequality\n",
    "L = rho + np.linalg.norm(np.dot(Q0.T, Q0))\n",
    "\n",
    "t = time()\n",
    "Pmin, n = gradient(lambda P : objective(P, Q0, R, mask, rho), P0, 1/L, 1.)\n",
    "print(\"Fixed step gradient descent : %d steps, time elapsed %0.3f s\"%(n, time()-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3) Algorithmical optimization for the problem with $Q_0$ fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can use the conjugate gradient method here because the function $g$ is a quadratic form : \n",
    "\n",
    "indeed, $ g(P) = \\frac{1}{2}\\left|\\left|\n",
    "\\mathbb{1}_K \\circ (R - Q^0P) \\right|\\right|_F^2 + \\frac{\\rho}{2}\\left|\\left|\n",
    "Q^0\\right|\\right|_F^2 + \\frac{\\rho}{2}\\|\n",
    "P \\|_F^2 = \\frac{1}{2} \\left( \\left(\\mathbb{1}_K \\circ (R - Q^0P)\\right)^T\\mathbb{1}_K \\circ(R - Q^0P)\\right) + \\frac{\\rho}{2}\\left( {Q^0}^TQ^0 \\right) + \\frac{\\rho}{2}\\left( P^TP \\right) = \\frac{1}{2} P^T \\left( {Q^0}^T \\mathbb{1}_K \\mathbb{1}_K^T {Q^0} + \\rho I\\right) P - \\frac{1}{2} \\left( P^T{Q^0}^T(\\mathbb{1}_K \\circ R) + (\\mathbb{1}_K \\circ R)^TQP \\right) + \\frac{1}{2}\\left( (\\mathbb{1}_K \\circ R)^T(\\mathbb{1}_K \\circ R) + \\rho {Q^0}^TQ^0\\right)$\n",
    "\n",
    "Therefore, $g(P) = \\frac{1}{2}P^TAP + b^TP + P^Tb + c$ with \n",
    "$$\\begin{align*} \n",
    "A &= {Q^0}^T (\\mathbb{1}_K \\mathbb{1}_K^T){Q^0} + \\rho I \\\\ \n",
    "b &= -\\frac{1}{2} {Q^0}^T(\\mathbb{1}_K \\circ R)\n",
    "\\\\ c &= \\frac{1}{2}\\left( (\\mathbb{1}_K \\circ R)^T(\\mathbb{1}_K \\circ R) + \\rho {Q^0}^TQ^0\\right)\n",
    "\\end{align*}$$\n",
    "\n",
    "Which provides a gradient $\\triangledown g(P) = AP + b +b^T$, and A being ysmetrical, positive definite, it allows for the application of *Fletcher & Reeves* algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed step gradient descent : 43 steps, time elapsed 1.899 s\n",
      "Linear gradient descent : 11 steps, time elapsed 2.652 s\n",
      "Conjugated gradients descent : 7 steps, time elapsed 0.518 s\n",
      "\n",
      "\n",
      "Relative error between fs and lin = 0.278 %\n",
      "Relative error between fs and conj = 0.332 %\n",
      "\n",
      "\n",
      "objective g(P,Q0) = 303938.100 for fixed step method\n",
      "objective g(P,Q0) = 303936.658 for linear method\n",
      "objective g(P,Q0) = 303936.586 for conjugate method\n"
     ]
    }
   ],
   "source": [
    "#tool  for Linear gradient descent : find slope.\n",
    "def find_gamma(P, f_and_grad, a, b):\n",
    "    k = 0\n",
    "    lgrad = f_and_grad(P)[1]\n",
    "    lfunc = f_and_grad(P)[0]\n",
    "    gamma = b*(a**k)\n",
    "    PP = P - gamma*lgrad\n",
    "    # other possible condition : f_and_grad(PP)[0] >= lfunc - 0.5*gamma*(np.linalg.norm(P-PP)**2)\n",
    "    while f_and_grad(PP)[0] >= lfunc + np.vdot(lgrad, (PP-P)) + 1/(2*gamma)*(np.linalg.norm(P-PP)**2) :\n",
    "        k += 1\n",
    "        gamma = b*(a**k)\n",
    "        PP = P - gamma*lgrad\n",
    "    return b*(a**(k-1))\n",
    "\n",
    "#Linear gradient descent using Armijo's line search\n",
    "def gradient_lin(f_and_grad, P0, epsilon, a=0.5, bb=0.5):\n",
    "    \"\"\"\n",
    "    executes a gradient descent with linear step\n",
    "    \n",
    "    Input:\n",
    "    f_and_grad is a tuple : returns both function and its gradient.\n",
    "    P0 = initialization point\n",
    "    epsilon : threshold for gradient variation\n",
    "    \"\"\"\n",
    "    P = P0.copy()\n",
    "    n = 0\n",
    "    gamma = bb/2\n",
    "    grad = f_and_grad(P)[1]\n",
    "    while np.linalg.norm(grad) > epsilon and n < 300 :\n",
    "        grad = f_and_grad(P)[1]\n",
    "        b = 2*gamma\n",
    "        gamma = find_gamma(P, f_and_grad, a, b)\n",
    "        P = P - gamma*grad\n",
    "        n+=1\n",
    "    return P, n \n",
    "\n",
    "# def gradient_conj(f_and_grad, A, P0, epsilon):\n",
    "#     \"\"\"\n",
    "#     executes a gradient descent with Fletcher & Reeves technique\n",
    "    \n",
    "#     Input:\n",
    "#     f_and_grad is a tuple : returns both function and its gradient.\n",
    "#     A = matrix in the standard expression of the quadratic form g\n",
    "#     P0 = initialization point\n",
    "#     epsilon : threshold for gradient variation\n",
    "#     \"\"\"\n",
    "#     P = P0\n",
    "#     n = 0\n",
    "#     g = f_and_grad(P)[1]\n",
    "#     d = - g\n",
    "#     s = - np.dot(d.T, g) / np.dot(d.T, np.dot(A, d)) #PROBLEM HERE as the dividend is not supposed to be a matrix\n",
    "#     P = P + np.dot(d,s)\n",
    "#     while np.linalg.norm(d) > epsilon and n < 300 :\n",
    "#         g = f_and_grad(P)[1]\n",
    "#         b = np.dot(d.T, np.dot(A, g)) / np.dot(d.T, np.dot(A, d))\n",
    "#         d = - g + np.dot(d,b)\n",
    "#         s = - np.dot(d.T, g) / np.dot(d.T, np.dot(A, d))\n",
    "#         P = P + np.dot(d,s)\n",
    "#         n+=1\n",
    "#     return P,n\n",
    "\n",
    "#The matrix A in the standard expression of the quadratic form g\n",
    "# A = 2*np.dot(Q0.T, np.dot(np.dot(mask, mask.T), Q0)) + rho*np.eye(C)\n",
    "# b = -0.5* np.dot(Q0.T, R*mask)\n",
    "# A = 2*np.dot(Q0.T, Q0) + rho*np.eye(C)\n",
    "\n",
    "#useful in conjugate gradient descent method\n",
    "def linear_operator(d, Q, mask):\n",
    "    tmp = (Q.dot(d)) * mask\n",
    "    Ad = rho * d + Q.T.dot(tmp)\n",
    "    return Ad\n",
    "\n",
    "def gradient_conj(f_and_grad, lin_op, P0, epsilon, max_iter=np.inf):\n",
    "    \"\"\"\n",
    "    executes a gradient descent with Fletcher & Reeves technique\n",
    "    \n",
    "    Input:\n",
    "    f_and_grad is a tuple : returns both function and its gradient.\n",
    "    lin_op = matrix in the standard expression of the quadratic form g, \n",
    "             adapted to the current problem where P is no longer a vector\n",
    "    P0 = initialization point\n",
    "    epsilon : threshold for gradient variation\n",
    "    max_iter : when to stop if gradient does not get below epsilon fast enough\n",
    "    \"\"\"\n",
    "    P = P0.copy() \n",
    "    n = 0\n",
    "\n",
    "    val, grad = f_and_grad(P)\n",
    "    d = - grad\n",
    "    Ad = lin_op(d)\n",
    "    s = - np.sum(d * grad) / np.sum(d * Ad)\n",
    "    P = P + s * d\n",
    "\n",
    "    while np.linalg.norm(grad) > epsilon and n <max_iter:\n",
    "        val, grad = f_and_grad(P)\n",
    "\n",
    "        b = np.sum(Ad * grad) / np.sum(d * Ad)\n",
    "        d = -grad + b * d\n",
    "        Ad = lin_op(d)\n",
    "        s = - np.sum(d * grad) / np.sum(d * Ad)\n",
    "        P = P + s * d\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    return P, n\n",
    "\n",
    "#Comparison of the time necessary for each technique to reach its solution\n",
    "t = time()\n",
    "Pmin, n = gradient(lambda P : objective(P, Q0, R, mask, rho), P0, 1/L, 1.)\n",
    "print(\"Fixed step gradient descent : %d steps, time elapsed %0.3f s\"%(n, time()-t))\n",
    "t = time()\n",
    "Pmin2, n = gradient_lin(lambda P : objective(P, Q0, R, mask, rho), P0, 1.)\n",
    "print(\"Linear gradient descent : %d steps, time elapsed %0.3f s\"%(n, time()-t))\n",
    "t = time()\n",
    "Pmin3, n = gradient_conj(lambda P : objective(P, Q0, R, mask, rho), lambda d: linear_operator(d, Q0, mask), P0, 1.)\n",
    "print(\"Conjugated gradients descent : %d steps, time elapsed %0.3f s\"%(n, time()-t))\n",
    "\n",
    "#comparison of the results\n",
    "print(\"\\n\")\n",
    "print(\"Relative error between fs and lin = %0.3f %%\" % (np.linalg.norm(Pmin-Pmin2)/np.linalg.norm(Pmin)*100))\n",
    "print(\"Relative error between fs and conj = %0.3f %%\" % (np.linalg.norm(Pmin-Pmin3)/np.linalg.norm(Pmin)*100))\n",
    "print('\\n')\n",
    "print(\"objective g(P,Q0) = %0.3f for fixed step method\"%(objective(Pmin, Q0, R, mask, rho)[0]))\n",
    "print(\"objective g(P,Q0) = %0.3f for linear method\"%(objective(Pmin2, Q0, R, mask, rho)[0]))\n",
    "print(\"objective g(P,Q0) = %0.3f for conjugate method\"%(objective(Pmin3, Q0, R, mask, rho)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "As a result, the most efficient techniques, as measured per the number of steps are in order : the conjugated gradients descent, the linear gradient descent, and the fixed step gradient descent. but linear gradient descent seems to be more time consuming despite that.\n",
    "\n",
    "\n",
    "## 4) Complete Solution\n",
    "\n",
    "When using the linear gradient descent technique, the output is a local minimum, not necessarily the global optimum as $g : (P,Q) \\mapsto g(P,Q)$ is not convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:31: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:32: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:224: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return reshape(newshape, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of the complete problem obtained after 210 steps, 57.875 s\n",
      "relative error = 263 %\n",
      "value of the objective function g(P,Q) = 32741.31229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:45: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "C:\\Users\\Benjamin\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:46: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "#same as objecive, but taking Q into account\n",
    "def total_objective(P, Q, R, mask, rho):\n",
    "    \"\"\"\n",
    "    g in the problem (objective function)\n",
    "    \n",
    "    Input:\n",
    "    P : Matrix input with shape C x I\n",
    "    Q : matrix with shape  U x C\n",
    "    R : matrix with shape U x I\n",
    "    mask : binary matrix with shape U x I\n",
    "    rho : strictly positive real\n",
    "\n",
    "    Output :\n",
    "    val : g(P, Q)\n",
    "    grad_P : obviously...\n",
    "    grad_Q : obviously...\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q.dot(P)) * mask\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q ** 2) + np.sum(P ** 2))\n",
    "    grad_P = -np.dot(Q.T, tmp) + rho*P\n",
    "    grad_Q = -np.dot(tmp, P.T) + rho*Q\n",
    "    \n",
    "    return val, grad_P, grad_Q\n",
    "\n",
    "#same as above, but vectorized, so that we can use the linear gradient solver\n",
    "def total_objective_vectorized(PQvec, R, mask, rho):\n",
    "    n_items = R.shape[1]\n",
    "    n_users = R.shape[0]\n",
    "    F = PQvec.shape[0] / (n_items + n_users)\n",
    "    Pvec = PQvec[0:n_items*F]\n",
    "    Qvec = PQvec[n_items*F:]\n",
    "    P = np.reshape(Pvec, (F, n_items))\n",
    "    Q = np.reshape(Qvec, (n_users, F))\n",
    "\n",
    "    val, grad_P, grad_Q = total_objective(P, Q, R, mask, rho)\n",
    "    return val, np.concatenate([grad_P.ravel(), grad_Q.ravel()])\n",
    "\n",
    "t = time()\n",
    "PQvec_sol, n = gradient_lin(lambda t : total_objective_vectorized(t, R, mask, rho), np.concatenate([P0.ravel(), Q0.ravel()]), 100)\n",
    "\n",
    "n_items = R.shape[1]\n",
    "n_users = R.shape[0]\n",
    "F = PQvec_sol.shape[0] / (n_items + n_users)\n",
    "Pvec_sol = PQvec_sol[0:n_items*F]\n",
    "Qvec_sol = PQvec_sol[n_items*F:]\n",
    "P_sol = np.reshape(Pvec_sol, (F, n_items))\n",
    "Q_sol = np.reshape(Qvec_sol, (n_users, F))\n",
    "\n",
    "print(\"result of the complete problem obtained after %d steps, %0.3f s\"%(n, time()-t))\n",
    "relative_error_sol = np.linalg.norm(R - np.dot(Q_sol, P_sol))/np.linalg.norm(R)*100\n",
    "print(\"relative error = %0.0f %%\"% relative_error_sol)\n",
    "print(\"value of the objective function g(P,Q) = %0.5f\"%(total_objective(P_sol, Q_sol, R, mask, rho)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Alternate Least Square solving\n",
    "\n",
    "It converges because :\n",
    "- at each step, the problem solved is convex. The $P$ or $Q$ found always exists, and makes the optimization function at least no greater than before computation.\n",
    "- So after each consecutive optimization of $P$ and $Q$, the optimization function has decreased (not striclty).\n",
    "\n",
    "As we obtain a monotonic function ($(P,Q) \\mapsto g(P,Q)$ decreases), that is bounded (because g takes values in $\\mathbb{R}_+$), the algorithm converges.\n",
    "\n",
    "**BEWARE:** takes quite a while, possibly _**more than 30min**_ !!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result of the complete problem obtained after 0 steps, 0.192 s\n",
      "relative error = 324 %\n",
      "value of the objective function g(P,Q) = 30134.47905 \n"
     ]
    }
   ],
   "source": [
    "def objective_Q(P0, Q, R, mask, rho):\n",
    "    \"\"\"\n",
    "    g in the problem (objective function)\n",
    "    \n",
    "    Input:\n",
    "    P0 : Matrix input with shape C x I\n",
    "    Q : matrix with shape  U x C\n",
    "    R : matrix with shape U x I\n",
    "    mask : binary matrix with shape U x I\n",
    "    rho : strictly positive real\n",
    "\n",
    "    Output :\n",
    "    val : g(Q)\n",
    "    grad_Q : obviously...\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = (R - Q.dot(P0)) * mask\n",
    "    val = np.sum(tmp ** 2)/2. + rho/2. * (np.sum(Q ** 2) + np.sum(P0 ** 2))\n",
    "    grad_Q = -np.dot(tmp, P0.T) + rho*Q\n",
    "    \n",
    "    return (val, grad_Q)\n",
    "\n",
    "def ALS(P0, Q0, R, mask, rho, epsilon, eps_in, N):\n",
    "    from progress_bar import InitBar\n",
    "    n = 0\n",
    "    P, Q = P0.copy(), Q0.copy()\n",
    "    _, grad_P, grad_Q = total_objective(P0, Q0, R, mask, rho)\n",
    "    \n",
    "#     bar = InitBar()\n",
    "   \n",
    "    while (np.linalg.norm(grad_P) >= epsilon or np.linalg.norm(grad_Q) >= epsilon) and n <= N:\n",
    "        P,_ = gradient_lin(lambda P : objective(P, Q, R, mask, rho), P, eps_in)\n",
    "        Q,_ = gradient_lin(lambda Q : objective_Q(P, Q, R, mask, rho), Q, eps_in)\n",
    "        val, grad_P, grad_Q = total_objective(P, Q, R, mask, rho)\n",
    "        n += 1\n",
    "        print(n, \"/\", N+1, \",  g(P,Q)= \",val, \",  grad_P = \",np.linalg.norm(grad_P), \",  grad_Q = \", np.linalg.norm(grad_Q))\n",
    "#         bar(n/N*100)\n",
    "\n",
    "#     del bar\n",
    "            \n",
    "    return P, Q, n\n",
    "\n",
    "# P_als.txt and Q_als.txt are files which store the best values minimizing g obtained during a previous iteration of \n",
    "# the algorithm. Instead of using it, one can consider the values P0 and Q0 as before\n",
    "\n",
    "t = time()\n",
    "P_als, Q_als, n = ALS(np.loadtxt('P_als.txt'), np.loadtxt('Q_als.txt'), R, mask, rho, 5., 5., 30)\n",
    "print(\"result of the complete problem obtained after %d steps, %0.3f s\"%(n, time()-t))\n",
    "relative_error_als = np.linalg.norm(R - np.dot(Q_als, P_als))/np.linalg.norm(R)*100\n",
    "print(\"relative error = %0.0f %%\"% relative_error_als)\n",
    "print(\"value of the objective function g(P,Q) = %0.5f \"%(total_objective(P_als, Q_als, R, mask, rho)[0]))\n",
    "\n",
    "#storing these values so that if we restart the algorithm, we have a better starting point.\n",
    "np.savetxt('P_als.txt', P_als)\n",
    "np.savetxt('Q_als.txt', Q_als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Comparison of ALS, and non convex solving using Linear Gradient Descent\n",
    "\n",
    "**According to the information printed before**\n",
    "- The computing time is much lower for LGD than for ALS\n",
    "- The precision is greater for ALS than for LGD ... but it is not obvious when comparing it with the same $\\epsilon = 100$ \n",
    "- The value of the objective function is lower for ALS than for LGD, but after many iterations (30 x 20)\n",
    "\n",
    "For ALS, as we come closer to a minimum, the computing time per step decreases a little.\n",
    "\n",
    "One can notice that the product PQ is farther from R with ALS than with LGD (this is what the relative error measures), however this is not incompatible with the fact the objective function is lower with ALS\n",
    "\n",
    "## Recommandation for user 449\n",
    "\n",
    "The aim is to suggest the ID of the movie, a movie he has not seen yet, to which user n°449 would give the greater rating, according to the matrix factorization $R = QP$.\n",
    "$\n",
    "R = \\underset{ Q : \\; size \\; = \\; U \\times C}{\\begin{bmatrix}\n",
    "  & & \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\ \\\\\n",
    "  & & \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\ \\\\\n",
    "  & & \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\ \\\\\n",
    "  & & \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\ \\\\\n",
    "  & & \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\ \\\\\n",
    "  & & User \\; n°499 \\; appreciation \\; for \\; each \\; characteristic \\; C_U  \\\\\n",
    "  & & \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\ \\\\\n",
    "  & & \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\ \\\\\n",
    " \\end{bmatrix}} \\times \n",
    " \\underset{ P : \\; size \\; = \\; C \\times I}{\\begin{bmatrix}\n",
    "  & | & | & | & Film \\; n°I & | & |& |& |&\\\\\n",
    "  & | & | & | & with \\; characteristics \\; C_I & | & |& |& |&\\\\\n",
    "  & | & | & | & & |& |& |& |&\\\\\n",
    "  & | & | & | & &|& |& |& |&\\\\\n",
    " \\end{bmatrix}}\n",
    " $\n",
    " \n",
    "So to find the movie best suited for user n°449, all we have to do is consider the line $Q_{449,:}$ which describes the appreciation for each movie characteristic ( something like drama, comedy, love story, violence, countries, etc.) and make the scalar product $\\left<Q_{449,:} \\;;\\; P\\right>^T$ which return $M_{449}$, a vector of size $I\\times1$ which represents the rating that user 449 would give for each movie.\n",
    "\n",
    "All what is left to do is selecting the movie with the highest rating, yet unseen by the user 449. Assuming there is at least one positive score in the list, it can be found by computing $M_{449}' = \\left<Q_{449,:} \\;;\\; P\\right>^T \\underset{element-wise}{*} \\left(1_{I \\times 1}^T - \\mathbb{1}_K^T \\right) $ where $1$ is a matrix made esclusively of '1'. The movie to be recommanded is the index of the greater value of this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of negatively rated movies: 0 out of 1682\n",
      "The movie I would suggest to user 449 is movie n° 1462 (out of the 1682 available ) (with rating 5.5224)\n",
      "\n",
      " list of best rated movies by user 449, indexed by movie ID to the left,        and predicted rating to the right (/100)\n",
      "[[1462  100]\n",
      " [ 908   98]\n",
      " [1063   93]\n",
      " [1448   93]\n",
      " [1274   92]\n",
      " [1366   92]\n",
      " [1174   91]\n",
      " [1642   91]\n",
      " [1251   90]\n",
      " [1241   90]\n",
      " [1175   90]\n",
      " [ 962   89]\n",
      " [1425   88]\n",
      " [1168   88]\n",
      " [ 407   87]\n",
      " [ 866   87]\n",
      " [1074   87]\n",
      " [ 250   86]\n",
      " [1193   86]\n",
      " [1232   86]]\n"
     ]
    }
   ],
   "source": [
    "m = np.dot(Q_als[449,:], P_als)\n",
    "print(\"Number of negatively rated movies: %d out of %d\"%(np.count_nonzero(m<0), m.shape[0]))\n",
    "m = m * (1 - mask[449,:])\n",
    "m = m.T\n",
    "movie_index = np.argmax(m)\n",
    "print(\"The movie I would suggest to user 449 is movie n° %d (out of the %d available ) (with rating %0.4f)\"%(movie_index, m.shape[0], m [movie_index]))\n",
    "\n",
    "# One can standardize m so as to attribute a mark, as a percentage, to each movie.\n",
    "Range = m[np.argmax(m)] - m[np.argmin(m)]\n",
    "m = (m - m[np.argmin(m)])/Range*100\n",
    "#print(\"standardized rating for the movie suggested = %0.2f %% \" %(m[movie_index]))\n",
    "print(\"\\n list of best rated movies by user 449, indexed by movie ID to the left,\\\n",
    "        and predicted rating to the right (/100)\" )\n",
    "\n",
    "list_of_best_movies = np.argsort(-m)\n",
    "sorted_ratings = np.zeros((m.shape[0], 2))\n",
    "sorted_ratings[:, 0], sorted_ratings[:, 1] = list_of_best_movies, m[list_of_best_movies]\n",
    "    \n",
    "print(np.round(sorted_ratings[:20], decimals=0).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#contact Roland BADEAU email telecom parisetch"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
